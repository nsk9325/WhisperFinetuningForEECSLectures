{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "1f54a5cd",
      "metadata": {
        "id": "1f54a5cd"
      },
      "source": [
        "# Whisper Small 다단계 파인튜닝 노트북\n",
        "\n",
        "이 노트북은 전처리된 **MIT 영어강의** 데이터셋으로 OpenAI **Whisper‑small** 모델을 두 단계로 파인튜닝합니다."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "43fadf00",
      "metadata": {
        "id": "43fadf00"
      },
      "source": [
        "## 1. 환경 준비"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d96c33ca",
      "metadata": {
        "id": "d96c33ca"
      },
      "outputs": [],
      "source": [
        "!pip install -q -U \"transformers==4.52.3\" datasets accelerate evaluate jiwer\n",
        "!pip install -q git+https://github.com/openai/whisper.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1cec45b6",
      "metadata": {
        "id": "1cec45b6"
      },
      "outputs": [],
      "source": [
        "from huggingface_hub import notebook_login\n",
        "notebook_login()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "521f29f1",
      "metadata": {
        "id": "521f29f1"
      },
      "source": [
        "## 2. 데이터 로드 & 안전한 검증 split 생성\n",
        "`validation` split 이 없는 경우 자동으로 `test` 또는 `train` 10 %를 사용합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "383238ea",
      "metadata": {
        "id": "383238ea"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "def get_eval_split(dataset_dict, pct=0.1):\n",
        "    # validation이 이미 있으면 그대로 리턴\n",
        "    if \"validation\" in dataset_dict:\n",
        "        return dataset_dict[\"validation\"]\n",
        "\n",
        "    # test가 있으면 test를 평가 데이터로 사용\n",
        "    if \"test\" in dataset_dict:\n",
        "        return dataset_dict[\"test\"]\n",
        "\n",
        "    # 둘 다 없으면 train을 pct 비율만큼 떼서 validation(test)으로 사용\n",
        "    print(f\"⚠️ 'validation' split이 없어 train의 {int(pct*100)}%를 eval로 사용합니다.\")\n",
        "    split = dataset_dict[\"train\"].train_test_split(test_size=pct, seed=42)\n",
        "    dataset_dict[\"train\"] = split[\"train\"]\n",
        "    return split[\"test\"]\n",
        "\n",
        "# ──────────────────────────────────────────────────────────────────────────\n",
        "\n",
        "MIT = load_dataset(\"yongjune2002/MITOCW-Whisper-Processed\")\n",
        "\n",
        "# 1) eval_ds를 먼저 만들어서 MIT[\"train\"]이 90%만 남도록 수정\n",
        "eval_ds = get_eval_split(MIT, pct=0.1)\n",
        "\n",
        "# 2) 이제 MIT[\"train\"]에는 “나머지 90%”만 남았으므로, 바로 train으로 사용하면 된다\n",
        "train_ds = MIT[\"train\"]\n",
        "\n",
        "# (필요하다면 train_ds에만 shuffle, map, tokenization 등 전처리 진행)\n",
        "print(train_ds)\n",
        "print(eval_ds)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a6c9a5be",
      "metadata": {
        "id": "a6c9a5be"
      },
      "source": [
        "## 3. 데이터 콜레이터"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4f430c25",
      "metadata": {
        "id": "4f430c25"
      },
      "outputs": [],
      "source": [
        "from dataclasses import dataclass\n",
        "from typing import List, Dict, Union\n",
        "import torch\n",
        "\n",
        "@dataclass\n",
        "class DataCollatorSpeechSeq2SeqWithPadding:\n",
        "    processor: any\n",
        "    def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]):\n",
        "        input_feats = [{\"input_features\": f[\"input_features\"]} for f in features]\n",
        "        label_feats = [{\"input_ids\": f[\"labels\"]} for f in features]\n",
        "        batch = self.processor.feature_extractor.pad(input_feats, return_tensors=\"pt\")\n",
        "        labels = self.processor.tokenizer.pad(label_feats, padding=True, return_tensors=\"pt\").input_ids\n",
        "        labels[labels == self.processor.tokenizer.pad_token_id] = -100\n",
        "        batch[\"labels\"] = labels\n",
        "        return batch"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a28aefa7",
      "metadata": {
        "id": "a28aefa7"
      },
      "source": [
        "## 4. 모델 & 프로세서 로드"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f6cfc473",
      "metadata": {
        "id": "f6cfc473"
      },
      "outputs": [],
      "source": [
        "from transformers import WhisperProcessor, WhisperForConditionalGeneration\n",
        "\n",
        "checkpoint = \"openai/whisper-small\"\n",
        "processor = WhisperProcessor.from_pretrained(checkpoint)\n",
        "processor.tokenizer.set_prefix_tokens(language=\"en\", task=\"transcribe\")\n",
        "model = WhisperForConditionalGeneration.from_pretrained(checkpoint)\n",
        "\n",
        "# 1단계: 인코더 freeze\n",
        "for p in model.model.encoder.parameters():\n",
        "    p.requires_grad = False"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(train_ds)"
      ],
      "metadata": {
        "id": "Y5fa2_7-RA6x"
      },
      "id": "Y5fa2_7-RA6x",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "8a62f573",
      "metadata": {
        "id": "8a62f573"
      },
      "source": [
        "## 5. 두 단계 학습 콜백"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "910ea525",
      "metadata": {
        "id": "910ea525"
      },
      "outputs": [],
      "source": [
        "from transformers import TrainerCallback\n",
        "\n",
        "class UnfreezeBottom2Callback(TrainerCallback):\n",
        "    def on_step_end(self, args, state, control, **kwargs):\n",
        "        if state.global_step == 100:\n",
        "            print(\"▶️  Unfreezing bottom 2 encoder layers …\")\n",
        "            for layer in model.model.encoder.layers[:2]:\n",
        "                for p in layer.parameters():\n",
        "                    p.requires_grad = True"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6b95664c",
      "metadata": {
        "id": "6b95664c"
      },
      "source": [
        "## 6. 트레이닝 인자"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2f865e18",
      "metadata": {
        "id": "2f865e18"
      },
      "outputs": [],
      "source": [
        "from transformers import Seq2SeqTrainingArguments\n",
        "\n",
        "training_args = Seq2SeqTrainingArguments(\n",
        "    output_dir=\"whisper-mit\",\n",
        "    per_device_train_batch_size=2,\n",
        "    gradient_accumulation_steps=8,\n",
        "    learning_rate=5e-6,\n",
        "    warmup_steps=50,\n",
        "    num_train_epochs=10,\n",
        "    weight_decay=0.01,\n",
        "    fp16=False,\n",
        "\n",
        "    eval_strategy=\"epoch\",           # 에폭이 끝날 때마다 평가\n",
        "    save_strategy=\"epoch\",           # 에폭이 끝날 때마다 저장\n",
        "\n",
        "    save_total_limit=1,\n",
        "\n",
        "    logging_strategy=\"steps\",\n",
        "    logging_steps=50,\n",
        "\n",
        "    gradient_checkpointing=True,\n",
        "    predict_with_generate=True,\n",
        "\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"wer\",\n",
        "    greater_is_better=False,\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2793f936",
      "metadata": {
        "id": "2793f936"
      },
      "source": [
        "## 7. 평가 함수: WER"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3a517459",
      "metadata": {
        "id": "3a517459"
      },
      "outputs": [],
      "source": [
        "import evaluate\n",
        "wer_metric = evaluate.load(\"wer\")\n",
        "\n",
        "def compute_metrics(pred):\n",
        "    pred_ids = pred.predictions\n",
        "    label_ids = pred.label_ids\n",
        "    pred_str = processor.batch_decode(pred_ids, skip_special_tokens=True)\n",
        "    label_str = processor.batch_decode(label_ids, skip_special_tokens=True)\n",
        "    return {\"wer\": wer_metric.compute(predictions=pred_str, references=label_str)}"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a326ee83",
      "metadata": {
        "id": "a326ee83"
      },
      "source": [
        "## 8. Trainer 초기화"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "40663c7c",
      "metadata": {
        "id": "40663c7c"
      },
      "outputs": [],
      "source": [
        "from transformers import Seq2SeqTrainer, EarlyStoppingCallback\n",
        "\n",
        "trainer = Seq2SeqTrainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_ds,\n",
        "    eval_dataset=eval_ds,\n",
        "    data_collator=DataCollatorSpeechSeq2SeqWithPadding(processor),\n",
        "    tokenizer=processor.tokenizer,\n",
        "    compute_metrics=compute_metrics,\n",
        "    callbacks=[\n",
        "        UnfreezeBottom2Callback(),\n",
        "        EarlyStoppingCallback(early_stopping_patience=2)\n",
        "    ],\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c2eeb856",
      "metadata": {
        "id": "c2eeb856"
      },
      "source": [
        "## 9. 학습 시작 🚀"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8f4b2785",
      "metadata": {
        "id": "8f4b2785"
      },
      "outputs": [],
      "source": [
        "trainer.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9e297ebb",
      "metadata": {
        "id": "9e297ebb"
      },
      "source": [
        "## 10. 모델 업로드"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "11238fda",
      "metadata": {
        "id": "11238fda"
      },
      "outputs": [],
      "source": [
        "from huggingface_hub import login\n",
        "login(token=\"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\")\n",
        "\n",
        "# 2) 모델 & 프로세서 Push\n",
        "#    repo_name은 \"<username>/<repo_id>\" 형태로 지정\n",
        "repo_name = \"tfbghjk/whisper-mit-small\"\n",
        "\n",
        "# 이미 정의해 두신 Trainer와 Processor 객체가 있다면\n",
        "trainer.push_to_hub(repo_name)\n",
        "processor.push_to_hub(repo_name)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.save_model()                         # ./whisper-mit/pytorch_model.bin 등 생성\n",
        "processor.save_pretrained(\"./whisper-mit\")   # processor 파일들(토크나이저) 생성"
      ],
      "metadata": {
        "id": "ka6fIQaws5Pz"
      },
      "id": "ka6fIQaws5Pz",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import WhisperProcessor, WhisperForConditionalGeneration\n",
        "from huggingface_hub import login\n",
        "login(token=\"허깅페이스 토큰 넣기\")\n",
        "\n",
        "\n",
        "local_model_dir = \"./whisper-mit\"\n",
        "repo_name = \"tfbghjk/whisper-mit-small_v2\"\n",
        "\n",
        "processor = WhisperProcessor.from_pretrained(local_model_dir)\n",
        "processor.push_to_hub(repo_name)\n",
        "\n",
        "model = WhisperForConditionalGeneration.from_pretrained(local_model_dir)\n",
        "model.push_to_hub(repo_name)\n",
        "\n",
        "from transformers import Trainer\n",
        "trainer.push_to_hub(repo_name)"
      ],
      "metadata": {
        "id": "a8z2x5vhsWyv"
      },
      "id": "a8z2x5vhsWyv",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "11ff3d5f",
      "metadata": {
        "id": "11ff3d5f"
      },
      "source": [
        "## 11. 간단 테스트"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bcd32bcf",
      "metadata": {
        "id": "bcd32bcf"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "sample = eval_ds[0]\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# 모델을 float32로 강제\n",
        "model = model.to(device).float()\n",
        "\n",
        "# input도 float32로 강제\n",
        "input_features = torch.tensor(sample['input_features']).unsqueeze(0).to(device).float()\n",
        "\n",
        "predicted_ids = model.generate(input_features)\n",
        "print(processor.decode(predicted_ids[0], skip_special_tokens=True))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b928b20a",
      "metadata": {
        "id": "b928b20a"
      },
      "source": [
        "**12. (선택) Gradio 데모**##"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gradio"
      ],
      "metadata": {
        "id": "a6DKxdA0cT7p"
      },
      "id": "a6DKxdA0cT7p",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**13. Evaluation**\n"
      ],
      "metadata": {
        "id": "UGhzFv1-t56m"
      },
      "id": "UGhzFv1-t56m"
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate after training\n",
        "results = trainer.evaluate()\n",
        "\n",
        "# Print the results (WER and other metrics)\n",
        "print(\"Evaluation results:\", results)"
      ],
      "metadata": {
        "id": "HmTcdgZguAFx"
      },
      "id": "HmTcdgZguAFx",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Comparing"
      ],
      "metadata": {
        "id": "Mjpr1xCJuWRy"
      },
      "id": "Mjpr1xCJuWRy"
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import WhisperForConditionalGeneration\n",
        "\n",
        "# 1️⃣ 사전 훈련된 모델 평가\n",
        "pretrained_model = WhisperForConditionalGeneration.from_pretrained(\"openai/whisper-small\").to(device).float()\n",
        "trainer.model = pretrained_model  # Trainer에 모델 할당\n",
        "pretrained_results = trainer.evaluate()\n",
        "print(\"Pretrained model WER:\", pretrained_results[\"eval_wer\"])\n",
        "\n",
        "# 2️⃣ 파인튜닝된 모델 평가\n",
        "trainer.model = model  # fine-tuned model로 교체\n",
        "fine_tuned_results = trainer.evaluate()\n",
        "print(\"Fine-tuned model WER:\", fine_tuned_results[\"eval_wer\"])\n",
        "\n",
        "# 3️⃣ 성능 비교\n",
        "improvement_in_wer = pretrained_results['eval_wer'] - fine_tuned_results['eval_wer']\n",
        "print(f\"WER improvement after fine-tuning: {improvement_in_wer:.4f}\")"
      ],
      "metadata": {
        "id": "BocJE2LpuYJn"
      },
      "id": "BocJE2LpuYJn",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchaudio\n",
        "import gradio as gr\n",
        "from transformers import WhisperProcessor, WhisperForConditionalGeneration\n",
        "\n",
        "# Hugging Face에서 pretrained whisper-small 로드\n",
        "checkpoint = \"openai/whisper-small\"\n",
        "processor = WhisperProcessor.from_pretrained(checkpoint)\n",
        "model = WhisperForConditionalGeneration.from_pretrained(checkpoint).to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "def transcribe(file):\n",
        "    # 파일에서 waveform 로드\n",
        "    speech_array, sampling_rate = torchaudio.load(file)\n",
        "\n",
        "    # Whisper는 16kHz 샘플링 기대 → 필요시 리샘플링\n",
        "    if sampling_rate != 16000:\n",
        "        resampler = torchaudio.transforms.Resample(orig_freq=sampling_rate, new_freq=16000)\n",
        "        speech_array = resampler(speech_array)\n",
        "\n",
        "    # Whisper feature 추출\n",
        "    input_features = processor.feature_extractor(\n",
        "        speech_array.squeeze().numpy(), sampling_rate=16000, return_tensors=\"pt\"\n",
        "    ).input_features.to(model.device)\n",
        "\n",
        "    # Whisper 모델로 예측\n",
        "    predicted_ids = model.generate(input_features)\n",
        "    transcription = processor.decode(predicted_ids[0], skip_special_tokens=True)\n",
        "\n",
        "    return transcription\n",
        "\n",
        "# Gradio 인터페이스 실행\n",
        "gr.Interface(fn=transcribe, inputs=gr.Audio(type=\"filepath\"), outputs=\"text\").launch()\n"
      ],
      "metadata": {
        "id": "6SwGQlyEksHE"
      },
      "id": "6SwGQlyEksHE",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchaudio\n",
        "import gradio as gr\n",
        "from transformers import WhisperProcessor, WhisperForConditionalGeneration\n",
        "\n",
        "checkpoint = \"tfbghjk/whisper-mit-small_v2\"\n",
        "\n",
        "# 1) Processor(토크나이저 + feature-extractor) 불러오기\n",
        "processor = WhisperProcessor.from_pretrained(checkpoint)\n",
        "processor.tokenizer.set_prefix_tokens(language=\"en\", task=\"transcribe\")\n",
        "\n",
        "# 2) 모델 불러오기 (GPU/CPU 할당)\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model = WhisperForConditionalGeneration.from_pretrained(checkpoint).to(device)\n",
        "\n",
        "model.generation_config.forced_decoder_ids = None\n",
        "model.config.forced_decoder_ids = None\n",
        "\n",
        "\n",
        "def transcribe(file):\n",
        "    speech_array, sampling_rate = torchaudio.load(file)\n",
        "    if sampling_rate != 16000:\n",
        "        resampler = torchaudio.transforms.Resample(orig_freq=sampling_rate, new_freq=16000)\n",
        "        speech_array = resampler(speech_array)\n",
        "\n",
        "    input_features = processor.feature_extractor(\n",
        "        speech_array.squeeze().numpy(),\n",
        "        sampling_rate=16000,\n",
        "        return_tensors=\"pt\"\n",
        "    ).input_features.to(device)\n",
        "\n",
        "    predicted_ids = model.generate(input_features)\n",
        "\n",
        "    transcription = processor.decode(predicted_ids[0], skip_special_tokens=True)\n",
        "    return transcription\n",
        "\n",
        "\n",
        "# 6) Gradio 인터페이스 실행\n",
        "gr.Interface(\n",
        "    fn=transcribe,\n",
        "    inputs=gr.Audio(type=\"filepath\"),\n",
        "    outputs=\"text\"\n",
        ").launch()\n"
      ],
      "metadata": {
        "id": "wb-BoX7U1kx5"
      },
      "id": "wb-BoX7U1kx5",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}